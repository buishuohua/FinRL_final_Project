{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "from env import StockLearningEnv\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_file/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据缓存\n",
      "数据缓存成功!\n"
     ]
    }
   ],
   "source": [
    "e_train_gym = StockLearningEnv(df = df, **config.ENV_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE|STEPS|TERMINAL_REASON|CASH           |TOT_ASSETS     |TERMINAL_REWARD|GAINLOSS_PCT|RETREAT_PROPORTION\n",
      "   0| 499|update         |￥26,549        |￥2,276,724     |113.61772%|127.67236%|-14.05%   \n",
      "   0| 999|update         |￥51,781        |￥1,741,219     |39.85222% |74.12192% |-34.27%   \n",
      "   0|1368|Last Date      |￥6,388         |￥1,690,649     |32.76649% |69.06486% |-36.30%   \n",
      "   1| 499|update         |￥67,210        |￥1,187,003     |16.73394% |18.70030% |-1.97%    \n",
      "   1| 999|update         |￥67,210        |￥1,806,196     |53.17972% |80.61961% |-27.44%   \n",
      "   1|1499|update         |￥67,210        |￥2,772,930     |174.45440%|177.29296%|-2.84%    \n",
      "   1|1740|Last Date      |￥67,210        |￥2,315,017     |108.28290%|131.50172%|-23.22%   \n"
     ]
    }
   ],
   "source": [
    "episode = 50000\n",
    "model = DDPG(policy='MlpPolicy', env=e_train_gym, **config.DDPG_PARAMS)\n",
    "model.learn(total_timesteps= episode)\n",
    "model.save(os.path.join('train_file', \"{}.model\".format('DDPG' + str(episode))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据缓存\n",
      "数据缓存成功!\n"
     ]
    }
   ],
   "source": [
    "trade_df = pd.read_csv('./data_file/trade.csv')\n",
    "e_trade_gym = StockLearningEnv(df = trade_df, **config.ENV_TRADE_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2ca1a5e5640>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode = 50000\n",
    "model = PPO(policy='MlpPolicy', env=e_trade_gym, **config.PPO_PARAMS)\n",
    "model.load(os.path.join('train_file', \"{}.model\".format('PPO' + str(episode))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE|STEPS|TERMINAL_REASON|CASH           |TOT_ASSETS     |TERMINAL_REWARD|GAINLOSS_PCT|RETREAT_PROPORTION\n",
      "回测完成!\n",
      "   1| 486|Last Date      |￥69,208        |￥1,539,112     |47.95132% |53.91116% |-5.96%    \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_env, test_obs = e_trade_gym.get_sb_env()\n",
    "\n",
    "\n",
    "test_env.reset()\n",
    "\n",
    "len_environment = len(e_trade_gym.df.index.unique())\n",
    "for i in range(len_environment):\n",
    "    action, _states = model.predict(test_obs)\n",
    "    test_obs, _, dones, _ = test_env.step(action)\n",
    "    if i == len_environment -2:\n",
    "        df_account = test_env.env_method(method_name=\"save_asset_memory\")[0]\n",
    "        df_action = test_env.env_method(method_name=\"save_action_memory\")[0]\n",
    "        print(\"回测完成!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'PPO'\n",
    "df_action.to_csv('./backtest/' + model_name + 'action.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account.to_csv('./backtest/' + model_name + 'account.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, in_dim, n_hidden_1, n_hidden_2, num_outputs):\n",
    "        super(Policy, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_dim, n_hidden_1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(n_hidden_1, n_hidden_2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(n_hidden_2, num_outputs)\n",
    "        )\n",
    "\n",
    "class Normal(nn.Module):\n",
    "    def __init__(self, num_outputs):\n",
    "        super().__init__()\n",
    "        self.stds = nn.Parameter(torch.zeros(num_outputs))\n",
    "    def forward(self, x):\n",
    "        dist = torch.distributions.Normal(loc=x, scale=self.stds.exp())\n",
    "        action = dist.sample()\n",
    "        return action\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    policy = Policy(4,20,20,5)\n",
    "    normal = Normal(5)\n",
    "    observation = torch.Tensor(4)\n",
    "    action = normal.forward(policy.layer( observation))\n",
    "    print(\"action: \",action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "947c325c4eb95a9946ab668956b3cfc6d347696304b060e14630bd99381a3fa4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
